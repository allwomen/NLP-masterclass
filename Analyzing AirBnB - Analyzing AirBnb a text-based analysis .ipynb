{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing AirBnb\n",
    "## A text-based analysis about Berlin's hospitality scene\n",
    "\n",
    "Airbnb has successfully disrupted the traditional hospitality industry as more and more travelers decide to use Airbnb as their primary accommodation provider. Since its inception in 2008, Airbnb has seen an enormous growth, with the number of rentals listed on its website growing exponentially each year.\n",
    "\n",
    "In Germany, no city is more popular than Berlin. That implies that Berlin is one of the hottest markets for Airbnb in Europe, with over 22,552 listings as of **November 2018**. With a size of 891 km², this means there are roughly 25 homes being rented out per km² in Berlin on Airbnb!\n",
    "\n",
    "The following question will drive this project:\n",
    "\n",
    "> **What do visitors like and dislike?**\n",
    "\n",
    "<br> We will process the reviews to find out what peoples' likes and dislikes are. We will use Natural Language Processing (NLP) and specifically **Sentiment Analysis** and **Topic Modeling**.\n",
    "\n",
    "### The datasets\n",
    "\n",
    "We will use the <a href='https://www.kaggle.com/brittabettendorf/berlin-airbnb-data'> reviews data </a> and combine it with some features from the detailed Berlin listings data, sourced from the Inside Airbnb website. Both datasets were scraped on November 07th 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<a id='Table of contents'></a>\n",
    "\n",
    "### <a href='#1. Obtaining and Viewing the Data'> 1. Obtaining and Viewing the Data </a>\n",
    "\n",
    "### <a href='#2. Preprocessing the Data'> 2. Preprocessing the Data </a>\n",
    "* <a href='#2.1. Dealing with Missing Values'> 2.1. Dealing with Missing Values </a>\n",
    "* <a href='#2.2. Language Detection'> 2.2. Language Detection </a>\n",
    "\n",
    "### <a href='#3. Visualizing the Data with WordClouds'> 3. Visualizing the Data with WordClouds </a>\n",
    "\n",
    "### <a href='#4. Sentiment Analysis'> 4. Sentiment Analysis </a>\n",
    "* <a href='#4.1. Get used to VADER package'> 4.1. Get used to VADER package </a>\n",
    "* <a href='#4.2. Calculating Sentiment Scores'> 4.2. Calculating Sentiment Scores </a>\n",
    "* <a href='#4.3. Comparing Negative and Positive Comments'> 4.3. Comparing Negative and Positive Comments </a>\n",
    "* <a href='#4.4. Investigating Positive Comments'> 4.4. Investigating Positive Comments </a>\n",
    "* <a href='#4.5. Investigating Negative Comments'> 4.5. Investigating Negative Comments </a>\n",
    "\n",
    "### <a href='#5. Topic Modeling'> 5. Topic Modeling </a>\n",
    "\n",
    "### <a href='#5. Appendix'> 6. Appendix </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtaining and Viewing the Data \n",
    "<a id='1. Obtaining and Viewing the Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15) \n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'reviews_summary.csv'\n",
    "reviews = pd.read_csv(filename)\n",
    "\n",
    "# checking shape ...\n",
    "print(\"The dataset has {} rows and {} columns.\".format(*reviews.shape))\n",
    "\n",
    "# ... and duplicates\n",
    "print(\"It contains {} duplicates.\".format(reviews.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it may be valuable to have more details, such as the latitude and longitude of the accommodation that has been reviewed, the neighbourhood it's in, the host id, etc. \n",
    "\n",
    "To get this information, let's **combine our reviews_dataframe** with the **listings_dataframe** and take only the columns we need from the latter one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'listings_summary.csv'\n",
    "listings = pd.read_csv(filename)\n",
    "\n",
    "# checking shape ...\n",
    "print(\"The dataset has {} rows and {} columns.\".format(*listings.shape))\n",
    "\n",
    "# ... and duplicates\n",
    "print(\"It contains {} duplicates.\".format(listings.duplicated().sum()))\n",
    "\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(reviews, listings[['neighbourhood_group_cleansed', 'host_id', 'latitude',\n",
    "                          'longitude', 'number_of_reviews', 'id', 'property_type']], \n",
    "              left_on='listing_id', right_on='id', how='left')\n",
    "\n",
    "df.rename(columns = {'id_x':'id', 'neighbourhood_group_cleansed':'neighbourhood_group'}, inplace=True)\n",
    "df.drop(['id_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset has {} rows and {} columns.\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hosts with many properties**\n",
    "\n",
    "By the way, let's find out if any private hosts have started to run a professional business through Airbnb - at least this is what was in the press. Let's work this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_per_host = pd.DataFrame(df.groupby('host_id')['listing_id'].nunique())\n",
    "\n",
    "properties_per_host.sort_values(by=['listing_id'], ascending=False, inplace=True)\n",
    "properties_per_host.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the top 3 hosts. How many properties do they have in the different areas? And are these private apartments, or something else, like a hostel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**> No. 1 Host**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_host = df.host_id == 1625771\n",
    "df[top1_host].neighbourhood_group.value_counts()\n",
    "\n",
    "pd.DataFrame(df[top1_host].groupby('neighbourhood_group')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df[top1_host].groupby('property_type')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This host owns apartments in 8 (!) districts. It looks like he was really able to deeply expand a well working business into different neighbourhoods..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**> No. 2 Host**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_host = df.host_id == 8250486\n",
    "df[top2_host].neighbourhood_group.value_counts()\n",
    "\n",
    "pd.DataFrame(df[top2_host].groupby('neighbourhood_group')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df[top2_host].groupby('property_type')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Well, looks like the second biggest player turned out to be a hostel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**> No. 3 Host**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_host = df.host_id == 2293972\n",
    "df[top3_host].neighbourhood_group.value_counts() #it prints it without being beautiful\n",
    "\n",
    "pd.DataFrame(df[top3_host].groupby('neighbourhood_group')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df[top3_host].groupby('property_type')['listing_id'].nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And host No. 3 also seems to be a professional lodging supplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "### 2. Preprocessing the Data \n",
    "<a id='2. Preprocessing the Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Dealing with Missing Values\n",
    "<a id='2.1. Dealing with Missing Values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "#### 2.2. Language Detection\n",
    "<a id='2.2. Language Detection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detection('Als Gregor Samsa eines Morgens aus unruhigen Träumen erwachte, fand er sich in seinem Bett zu einem ungeheueren Ungeziefer verwandelt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detection('It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell may take a long time, you can load the processed dataset in the next cell\n",
    "# df['language'] = df['comments'].apply(language_detection)\n",
    "# df.to_csv('language_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'language_processed.csv'\n",
    "df_lang = pd.read_csv(filename)\n",
    "df_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_lang.language.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_lang.language.value_counts(normalize=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df_lang.language.value_counts().head(6).sort_values().plot(kind='barh', figsize=(10,5),color=\"lightcoral\");\n",
    "plot.set_title(\"\\nWhat are the most frequent languages comments are written in?\\n\", \n",
    "             fontsize=30,fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_lang[(df_lang['language']=='en')]\n",
    "df_de  = df_lang[(df_lang['language']=='de')]\n",
    "df_fr  = df_lang[(df_lang['language']=='fr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "### 3. Visualizing the Data with WordClouds\n",
    "<a id='3. Visualizing the Data with WordClouds'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(string, language, title):\n",
    "    \n",
    "    # Generate WordCloud\n",
    "    wordcloud = WordCloud(max_words=200, background_color=\"black\", \n",
    "                      width=3000, height=2000,\n",
    "                      stopwords=stopwords.words(language)).generate(string)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=18, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**English WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_comments = str(df_eng.comments.values).lower()\n",
    "plot_wordcloud(english_comments, 'english', 'English Comments\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**German WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "german_comments = str(df_de.comments.values).lower()\n",
    "plot_wordcloud(german_comments, 'german', 'German Comments\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**French WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_comments = str(df_fr.comments.values).lower()\n",
    "plot_wordcloud(french_comments, 'french', 'French Comments\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "### 4. Sentiment Analysis\n",
    "<a id='4. Sentiment Analysis'></a>\n",
    "\n",
    "**Sentiment Analysis** tries to identify and extract **opinions** within a given text. The aim of sentiment analysis is to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "\n",
    "Often applied to reviews (products, restaurants…), survey data or any user generated content that can carry opinions (e.g. tweets)\n",
    "\n",
    "*“I loved the movie Parasite, it really deserved the Oscar”* -> Positive\n",
    "\n",
    "*“I didn’t like the staff’s rude attitude”* -> Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Get used to VADER package\n",
    "<a id='4.1. Get used to VADER package'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER: Valence Aware Dictionary and Sentiment Reasoner\n",
    "\n",
    "VADER belongs to a type of sentiment analysis that is based on **lexicons** of sentiment-related words. In this approach, each of the words in the lexicon is rated as positive or negative, and in many cases, **how** positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer_scores(\"I AM HAPPY :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER produces four sentiment metrics from these word ratings, which you can see above. The first three - positive, neutral and negative - represent the proportion of the text that falls into those categories. \n",
    "\n",
    "The final metric, **the compound score**, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "#### 4.2. Calculating Sentiment Scores\n",
    "<a id='4.2. Calculating Sentiment Scores'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have VADER produce all four scores for each of our English-language comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    score_list = list(score.values())\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader(\"I am happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell may take a long time, you can load the processed dataset in the next cell\n",
    "# df_eng['sentiment_pos'], df_eng['sentiment_neg'], df_eng['sentiment_neu'], df_eng['sentiment_compound'] = zip(*df_eng['comments'].map(vader))\n",
    "# df_eng.to_csv('df_en_sen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_sen = pd.read_csv('df_en_sen.csv')\n",
    "df_en_sen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape ...\n",
    "print(\"The dataset has {} rows and {} columns.\".format(*df_en_sen.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the distribution of all scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, figsize=(7,10))\n",
    "\n",
    "# plot all 3 histograms\n",
    "df_en_sen.hist('sentiment_neg', bins=25, ax=axes[0], color='lightcoral', alpha=0.6)\n",
    "axes[0].set_title('Negative Sentiment Score', fontsize=15)\n",
    "df_en_sen.hist('sentiment_neu', bins=25, ax=axes[1], color='lightsteelblue', alpha=0.6)\n",
    "axes[1].set_title('Neutral Sentiment Score', fontsize=15)\n",
    "df_en_sen.hist('sentiment_pos', bins=25, ax=axes[2], color='chartreuse', alpha=0.6)\n",
    "axes[2].set_title('Positive Sentiment Score', fontsize=15)\n",
    "\n",
    "# plot common x- and y-label\n",
    "fig.text(0.5, 0.04, 'Sentiment Scores',  fontweight='bold', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Number of Reviews', fontweight='bold', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# plot title\n",
    "plt.suptitle('Sentiment Analysis of Airbnb Reviews for Berlin\\n\\n', fontsize=20, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_sen.hist('sentiment_compound', bins=25, color='orange', alpha=0.6)\n",
    "\n",
    "# plot title\n",
    "plt.suptitle('Compound Sentiment Analysis of Airbnb Reviews for Berlin\\n\\n', fontsize=20, fontweight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the bulk of the reviews are tremendously positive. Wouldn't it be interesting to know what the negative and positive comments are about? Let's have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "#### 4.3. Comparing Negative and Positive Comments\n",
    "<a id='4.3. Comparing Negative and Positive Comments'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_en_sen.loc[df_en_sen.sentiment_compound >= 0.95]\n",
    "\n",
    "pos_comments = df_pos['comments'].tolist()\n",
    "len(pos_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = df_en_sen.loc[df_en_sen.sentiment_compound < 0.0]\n",
    "\n",
    "neg_comments = df_neg['comments'].tolist()\n",
    "len(neg_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the length of both positive and negative comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['text_length'] = df_pos['comments'].apply(len)\n",
    "df_neg['text_length'] = df_neg['comments'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "sns.distplot(df_pos['text_length'], bins=50, color='chartreuse')\n",
    "sns.distplot(df_neg['text_length'], bins=50, color='lightcoral')\n",
    "\n",
    "plt.title('\\nDistribution Plot for Length of Comments\\n')\n",
    "plt.legend(['Positive Comments', 'Negative Comments'])\n",
    "plt.xlabel('\\nText Length')\n",
    "plt.ylabel('Percentage of Comments\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode for the text length of positive comments can be found more to the right than for the negative comments, which means most of the positive comments are longer than most of the negative comments. But the tail for negative comments is thicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(pos_comments[10:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(neg_comments[10:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check if a scatter plot may reveal some differences in the comments' sentiment with respect to the districts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "cmap = sns.cubehelix_palette(rot=-.4, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(11,7))\n",
    "\n",
    "ax = sns.scatterplot(x=\"longitude\", y=\"latitude\", size='number_of_reviews', sizes=(5, 200),\n",
    "                     hue='sentiment_compound', palette=cmap,  data=df_en_sen)\n",
    "ax.legend(bbox_to_anchor=(1.3, 1), borderaxespad=0.)\n",
    "plt.title('\\nAccommodations in Berlin by Number of Reviwws & Sentiment\\n', fontsize=12, fontweight='bold')\n",
    "\n",
    "sns.despine(ax=ax, top=True, right=True, left=True, bottom=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "#### 4.4. Investigating Positive Comments\n",
    "<a id='4.4. Investigating Positive Comments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordCloud**\n",
    "\n",
    "After reading some of these reviews to get a feeling for what visitors applaud or complain about, WordClouds are a great tool to help us peek behind the curtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(str(pos_comments[0:3000]).lower(), 'english', 'Positively Tuned\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Distribution**\n",
    "\n",
    "Another method for visually exploring text is with frequency distributions. In the context of a text corpus, such a distribution tells us the prevalence of certain words. Here we use the Yellowbrick library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text.freqdist import FreqDistVisualizer\n",
    "from yellowbrick.style import set_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing text\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "docs = vectorizer.fit_transform(pos_comments)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# preparing the plot\n",
    "set_palette('pastel')\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title('The Top 30 most frequent words used in POSITIVE comments\\n', fontweight='bold', fontsize=20)\n",
    "\n",
    "# instantiating and fitting the FreqDistVisualizer, plotting the top 30 most frequent terms\n",
    "visualizer = FreqDistVisualizer(features=features, n=30)\n",
    "visualizer.fit(docs)\n",
    "visualizer.poof;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "### 5. Topic Modeling\n",
    "<a id='5. Topic Modeling'></a>\n",
    "Next we'll explore **topic modeling**, an unsupervised machine learning technique for abstracting topics from collections of documents or, in our case, for identifying which topic is being discussed in a comment. \n",
    "\n",
    "Put simply: \n",
    "* A document can be represented using a set of topics.\n",
    "* Each topic is represented as a set of words with their probabilities of occurring in that topic\n",
    "\n",
    "Methods for topic modeling have evolved significantly over the last decade. In this section, we'll explore a technique called *Latent Dirichlet Allocation (LDA)*, a widely used topic modelling technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "\n",
    "#### 5.1 Cleaning and Preprocessing\n",
    "<a id='5.1 Cleaning and Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n",
    "    punc_free = \"\".join(token for token in stop_free if token not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(comment).split() for comment in pos_comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "\n",
    "#### 5.2. Building the model\n",
    "<a id='5.2 Building the model'></a>\n",
    "First, we create a Gensim dictionary from the normalized data, then we convert this to a bag-of-words corpus, and save both dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import pickle \n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# let LDA find 3 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel3 = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "ldamodel.save('lda_3_topics.gensim')\n",
    "\n",
    "topics3 = ldamodel3.print_topics(num_words=10)\n",
    "for topic in topics3:\n",
    "    print(topic,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first topic includes words like *bed*, *also*, *even*, and a mysterious *u* (perhaps u-bahn for the underground?). It seems unclear to me what this was supposed to be about. \n",
    "- The second topic combines words like *great*, *place*, *stay*, and *recommend*, which sounds like a cluster related to overall satisfaction with the home.\n",
    "- The third topic includes words like *apartment*, *great*, and *location*, and *minute*. This sounds like a topic related to convenient distances from the accommodation to wherever something interesting was to go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let LDA find 5 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel5 = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "ldamodel5.save('lda_5_topics.gensim')\n",
    "\n",
    "topics5 = ldamodel5.print_topics(num_words=4)\n",
    "for topic in topics5:\n",
    "    print(topic, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally 10 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel10 = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "ldamodel10.save('lda_10_topics.gensim')\n",
    "\n",
    "topics10 = ldamodel10.print_topics(num_words=4)\n",
    "for topic in topics10:\n",
    "    print(topic, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together - the WordCloud, the Frequency Distribution and the Topic Modelling - it is often the following criteria that make someone rate an apartment **positively:**\n",
    "1. **The apartment is clean, the bathroom is clean, the bed is comfortable.**\n",
    "2. **The apartment is quiet and conducive to getting sound sleep.**\n",
    "3. **The area is centrally located with short walking distances, good public transport connections, and has cafes and restaurants nearby.**\n",
    "\n",
    "Apparently, getting the last two means trying to square the circle... but this is true for tourists all over the world.\n",
    "\n",
    "Before we move on to the negative comments, let's visualize the LDA model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. Visualizing topics*\n",
    "\n",
    "The pyLDAvis library is designed to provide a visual interface for interpreting the topics derived from a topic model by extracting information from a fitted LDA topic model.\n",
    "\n",
    "***The following code should be run locally only!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing 3 topics\n",
    "lda_display3 = pyLDAvis.gensim.prepare(ldamodel3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(lda_display3, 'lda_3_topics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the interface provides:\n",
    "\n",
    "- a left panel that depicts a global view of the model (how prevalent each topic is and how topics relate to each other);\n",
    "- a right panel containing a bar chart – the bars represent the terms that are most useful in interpreting the topic currently selected (what the meaning of each topic is).\n",
    "\n",
    "On the left, the topics are plotted as circles, whose centers are defined by the computed distance between topics (projected into 2 dimensions). The prevalence of each topic is indicated by the circle’s area. On the right, two juxtaposed bars show the topic-specific frequency of each term (in red) and the corpus-wide frequency (in blueish gray). When no topic is selected, the right panel displays the top 30 most salient terms for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing 5 topics\n",
    "lda_display5 = pyLDAvis.gensim.prepare(ldamodel5, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display5)\n",
    "pyLDAvis.save_html(lda_display5, 'lda_5_topics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "#### 4.5. Investigating Negative Comments\n",
    "<a id='4.5. Investigating Negative Comments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(str(neg_comments).lower(),'english', '\\nNegatively Tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing text\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "docs = vectorizer.fit_transform(neg_comments)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# preparing the plot\n",
    "set_palette('pastel')\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title('The Top 30 most frequent words used in NEGATIVE comments\\n', fontweight='bold')\n",
    "\n",
    "# instantiating and fitting the FreqDistVisualizer, plotting the top 30 most frequent terms\n",
    "visualizer = FreqDistVisualizer(features=features, n=30)\n",
    "visualizer.fit(docs)\n",
    "visualizer.poof;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Cleaning and Preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_neg = [clean(comment).split() for comment in neg_comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. LDA the Gensim way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_neg = corpora.Dictionary(doc_clean_neg)\n",
    "corpus_neg = [dictionary_neg.doc2bow(text) for text in doc_clean_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('corpus_neg.pkl', 'wb'))\n",
    "dictionary.save('dictionary_neg.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let LDA find 3 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel3_neg = gensim.models.ldamodel.LdaModel(corpus_neg, num_topics=3, id2word=dictionary_neg, passes=15)\n",
    "ldamodel3_neg.save('lda_3_topics_neg.gensim')\n",
    "\n",
    "topics3_neg = ldamodel3_neg.print_topics(num_words=10)\n",
    "for topic in topics3_neg:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let LDA find 5 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel5_neg = gensim.models.ldamodel.LdaModel(corpus_neg, num_topics=5, id2word=dictionary_neg, passes=15)\n",
    "ldamodel5_neg.save('lda_5_topics_neg.gensim')\n",
    "\n",
    "ldamodel5_neg = gensim.models.ldamodel.LdaModel.load('lda_5_topics_neg.gensim')\n",
    "topics5_neg = ldamodel5_neg.print_topics(num_words=4)\n",
    "for topic in topics5_neg:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally 10 topics\n",
    "# running this cell may take a long time\n",
    "ldamodel10_neg = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "ldamodel10_neg.save('lda_10_topics_neg.gensim')\n",
    "\n",
    "ldamodel10_neg = gensim.models.ldamodel.LdaModel.load('lda_10_topics_neg.gensim')\n",
    "topics10_neg = ldamodel10_neg.print_topics(num_words=4)\n",
    "for topic in topics10_neg:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's put all of the visualizations together and summarize what makes someone rate an apartment **negatively:**\n",
    "1. **The apartment and/or bathroom (especially the shower) are dirty.**\n",
    "2. **Problems in communicating with the host, e.g. one-sided cancellations by the host or to not being able to get a hold of him/her when having issues.**\n",
    "3. **The area is too far away from public transport connections or doesn't meet vistors' expectations in some way.**\n",
    "\n",
    "Before we finish analyzing the negative comments, let's visualize the LDA model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. Visualizing topics*\n",
    "\n",
    "***The following code should be run locally only!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing 3 topics\n",
    "lda_display3_neg = pyLDAvis.gensim.prepare(ldamodel3_neg, corpus_neg, dictionary_neg, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3_neg)\n",
    "pyLDAvis.save_html(lda_display3_neg, 'lda_3_topics_neg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing 5 topics\n",
    "lda_display5_neg = pyLDAvis.gensim.prepare(ldamodel5_neg, corpus_neg, dictionary_neg, sort_topics=False)\n",
    "pyLDAvis.display(lda_display5_neg)\n",
    "pyLDAvis.save_html(lda_display5_neg, 'lda_5_topics_neg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back to: <a href='#Table of contents'> Table of contents</a>*\n",
    "### 6. Appendix \n",
    "<a id='6. Appendix'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All resources used in this notebook are listed below.\n",
    "\n",
    "Data\n",
    "- Inside Airbnb: http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "WordClouds\n",
    "- https://vprusso.github.io/blog/2018/natural-language-processing-python-3/\n",
    "- https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "\n",
    "Bar Charts\n",
    "- http://robertmitchellv.com/blog-bar-chart-annotations-pandas-mpl.html\n",
    "\n",
    "YellowBrick Visualization\n",
    "- http://www.scikit-yb.org/en/latest/index.html\n",
    "\n",
    "Language Detection\n",
    "- TextBlob:\n",
    "    - https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "    - https://github.com/shubhamjn1/TextBlob/blob/master/Textblob.ipynb\n",
    "    - https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe\n",
    "    - https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "<br>\n",
    "- Spacy:\n",
    "    - https://github.com/nickdavidhaynes/spacy-cld\n",
    "    - https://spacy.io/usage/models\n",
    "<br>\n",
    "- Langdetect & LangId:\n",
    "    - https://pypi.org/project/langdetect/ \n",
    "    - https://www.probytes.net/blog/python-language-detection/\n",
    "    - https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/8-1-The-langdetect-and-langid-Libraries.ipynb\n",
    "\n",
    "Sentiment Analysis\n",
    "- *\"Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning\"* (Paperback) by B. Bengfort, R. Bilbro, T. Ojeda, published by O′Reilly\n",
    "- Jodie Burchell: http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "- Jodie Burchell: http://t-redactyl.io/blog/2017/01/how-do-we-feel-about-new-years-resolutions-according-to-sentiment-analysis.html\n",
    "- Jodie Burchell: https://github.com/t-redactyl/Blog-posts/blob/master/2017-04-15-sentiment-analysis-in-vader-and-twitter-api.ipynb\n",
    "- http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n",
    "\n",
    "- Susan Li: https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3\n",
    "- Sakshi Gupta (in R): https://towardsdatascience.com/uncovering-hidden-trends-in-airbnb-reviews-11eb924f2fec\n",
    "- Dmytro Iakubovskyi: https://towardsdatascience.com/digging-into-airbnb-data-reviews-sentiments-superhosts-and-prices-prediction-part1-6c80ccb26c6a\n",
    "- Dmytro Iakubovskyi: https://github.com/Dima806/Airbnb_project/blob/master/airbnb_final_analysis_v3.ipynb\n",
    "- Maurizio Santamicone: https://medium.com/@mauriziosantamicone/seattle-confidential-unpacking-airbnb-reviews-with-sentiment-d421c15d8b8f\n",
    "- Zhenyu: https://www.kaggle.com/zhenyufan/nlp-for-yelp-reviews/notebook?utm_medium=email&utm_source=intercom&utm_campaign=datanotes-2019\n",
    "\n",
    "Topic Modeling / LDA\n",
    "- Analytics Vidhya: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "- Susan Li: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "- https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "- https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/\n",
    "\n",
    "Diverse\n",
    "- https://data-viz-for-fun.com/2018/08/airbnb-data-viz/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
